{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark=SparkSession.builder.appName(\"RDD Trasformation & Actions\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('b', 1), ('a', 1), ('c', 1)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MAP Return a new RDD by applying a function to each element of this RDD\n",
    "x = spark.sparkContext.parallelize([\"b\", \"a\", \"c\"])\n",
    "y = x.map(lambda z: (z, 1))\n",
    "print(x.collect())\n",
    "print(y.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3]\n",
      "[2]\n"
     ]
    }
   ],
   "source": [
    "# Filter Return a new RDD containing only the elements that satisfy a predicate\n",
    "x = spark.sparkContext.parallelize([1,2,3])\n",
    "y = x.filter(lambda x: x%2 == 0) #keep even values\n",
    "print(x.collect())\n",
    "print(y.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3]\n",
      "[1, 100, 42, 2, 200, 42, 3, 300, 42]\n"
     ]
    }
   ],
   "source": [
    "# FlatMap Return a new RDD by first applying a function to all elements of this RDD, and then flattening the results\n",
    "x = spark.sparkContext.parallelize([1,2,3])\n",
    "y = x.flatMap(lambda x: (x, x*100, 42))\n",
    "print(x.collect())\n",
    "print(y.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('J', ['John', 'James']), ('F', ['Fred']), ('A', ['Anna'])]\n"
     ]
    }
   ],
   "source": [
    "#Group By Group the data in the original RDD. Create pairs where the key is the output of\n",
    "# a user function, and the value is all items for which the function yields this key.\n",
    "x = spark.sparkContext.parallelize(['John', 'Fred', 'Anna', 'James'])\n",
    "y = x.groupBy(lambda w: w[0])\n",
    "print([(k, list(v)) for (k, v) in y.collect()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('B', 5), ('B', 4), ('A', 3), ('A', 2), ('A', 1)]\n",
      "[('B', [5, 4]), ('A', [3, 2, 1])]\n"
     ]
    }
   ],
   "source": [
    "# Group By Key  Group the values for each key in the original RDD. Create a new pair where the\n",
    "# original key corresponds to this collected group of values.\n",
    "x = spark.sparkContext.parallelize([('B',5),('B',4),('A',3),('A',2),('A',1)])\n",
    "y = x.groupByKey()\n",
    "print(x.collect())\n",
    "print(list((j[0], list(j[1])) for j in y.collect()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1], [2, 3]]\n",
      "[[1, 42], [5, 42]]\n"
     ]
    }
   ],
   "source": [
    "#MapPartitions Return a new RDD by applying a function to each partition of this RDD\n",
    "x = spark.sparkContext.parallelize([1,2,3], 2)\n",
    "def f(iterator): yield sum(iterator); yield 42\n",
    "y = x.mapPartitions(f)\n",
    "# glom() flattens elements on the same partition\n",
    "print(x.glom().collect())\n",
    "print(y.glom().collect())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1], [2, 3]]\n",
      "[[(0, 1)], [(1, 5)]]\n"
     ]
    }
   ],
   "source": [
    "#Return a new RDD by applying a function to each partition of this RDD,\n",
    "#while tracking the index of the original partition\n",
    "x = spark.sparkContext.parallelize([1,2,3], 2)\n",
    "def f(partitionIndex, iterator): yield (partitionIndex, sum(iterator))\n",
    "y = x.mapPartitionsWithIndex(f)\n",
    "# glom() flattens elements on the same partition\n",
    "print(x.glom().collect())\n",
    "print(y.glom().collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5]\n",
      "[1]\n"
     ]
    }
   ],
   "source": [
    "# sample Return a new RDD containing a statistical sample of the original RDD\n",
    "x = spark.sparkContext.parallelize([1, 2, 3, 4, 5])\n",
    "y = x.sample(False,.4,42)\n",
    "print(x.collect())\n",
    "print(y.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1], [2, 3], [3], [4]]\n"
     ]
    }
   ],
   "source": [
    "#Union Return a new RDD containing all items from two original RDDs. Duplicates are not culled.\n",
    "x = spark.sparkContext.parallelize([1,2,3], 2)\n",
    "y = spark.sparkContext.parallelize([3,4], 2)\n",
    "z = x.union(y)\n",
    "print(z.glom().collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('a', (1, 3)), ('a', (1, 4)), ('b', (2, 5))]\n"
     ]
    }
   ],
   "source": [
    "# Join Return a new RDD containing all pairs of elements having the same key in the original RDDs\n",
    "x = spark.sparkContext.parallelize([(\"a\", 1), (\"b\", 2)])\n",
    "y = spark.sparkContext.parallelize([(\"a\", 3), (\"a\", 4), (\"b\", 5)])\n",
    "z = x.join(y)\n",
    "print(z.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 1, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "# distinct Return a new RDD containing distinct items from the original RDD (omitting all duplicates)\n",
    "x = spark.sparkContext.parallelize([1,2,3,3,4])\n",
    "y = x.distinct()\n",
    "print(y.collect())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1], [2, 3], [4, 5]]\n",
      "[[1], [2, 3, 4, 5]]\n"
     ]
    }
   ],
   "source": [
    "#coalesce Return a new RDD which is reduced to a smaller number of partitions\n",
    "x = spark.sparkContext.parallelize([1, 2, 3, 4, 5], 3)\n",
    "y = x.coalesce(2)\n",
    "print(x.glom().collect())\n",
    "print(y.glom().collect())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('J', 'John'), ('F', 'Fred'), ('A', 'Anna'), ('J', 'James')]\n"
     ]
    }
   ],
   "source": [
    "# Key BY Create a Pair RDD, forming one pair for each item in the original RDD. The\n",
    "# pair’s key is calculated from the value via a user-supplied function.\n",
    "x = spark.sparkContext.parallelize(['John', 'Fred', 'Anna', 'James'])\n",
    "y = x.keyBy(lambda w: w[0])\n",
    "print(y.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('J', 'James')], [('F', 'Fred')], [('A', 'Anna'), ('J', 'John')]]\n",
      "[[('F', 'Fred'), ('A', 'Anna')], [('J', 'James'), ('J', 'John')]]\n"
     ]
    }
   ],
   "source": [
    "#Return a new RDD with the specified number of partitions, placing original\n",
    "#items into the partition returned by a user supplied function\n",
    "x = spark.sparkContext.parallelize([('J','James'),('F','Fred'),('A','Anna'),('J','John')], 3)\n",
    "y = x.partitionBy(2, lambda w: 0 if w[0] < 'H' else 1)\n",
    "print(x.glom().collect())\n",
    "print(y.glom().collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 1), (2, 4), (3, 9)]\n"
     ]
    }
   ],
   "source": [
    "# zip Return a new RDD containing pairs whose key is the item in the original RDD, and whose\n",
    "#value is that item’s corresponding element (same partition, same index) in a second RDD\n",
    "x = spark.sparkContext.parallelize([1, 2, 3])\n",
    "y = x.map(lambda n:n*n)\n",
    "z = x.zip(y)\n",
    "print(z.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1], [2, 3]]\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "#getNumPartitions Return the number of partitions in RDD\n",
    "x = spark.sparkContext.parallelize([1,2,3], 2)\n",
    "y = x.getNumPartitions()\n",
    "print(x.glom().collect())\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1], [2, 3]]\n",
      "[1, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "# collect Return all items in the RDD to the driver in a single list\n",
    "x = spark.sparkContext.parallelize([1,2,3], 2)\n",
    "y = x.collect()\n",
    "print(x.glom().collect())\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4]\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "# reduce Aggregate all the elements of the RDD by applying a user function\n",
    "# pairwise to elements and partial results, and returns a result to the driver\n",
    "x = spark.sparkContext.parallelize([1,2,3,4])\n",
    "y = x.reduce(lambda a,b: a+b)\n",
    "print(x.collect())\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([1, 2, 3, 4], 10)\n"
     ]
    }
   ],
   "source": [
    "#Aggregate all the elements of the RDD by:\n",
    "#- applying a user function to combine elements with user-supplied objects,\n",
    "#- then combining those user-defined results via a second user function,\n",
    "#- and finally returning a result to the driver.\n",
    "\n",
    "seqOp = lambda data, item: (data[0] + [item], data[1] + item)\n",
    "combOp = lambda d1, d2: (d1[0] + d2[0], d1[1] + d2[1])\n",
    "x = spark.sparkContext.parallelize([1,2,3,4])\n",
    "y = x.aggregate(([], 0), seqOp, combOp)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 4, 1]\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "#Max Return the maximum item in the RDD\n",
    "x = spark.sparkContext.parallelize([2,4,1])\n",
    "y = x.max()\n",
    "print(x.collect())\n",
    "print(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 4, 1]\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "#Return the sum of the items in the RDD\n",
    "x = spark.sparkContext.parallelize([2,4,1])\n",
    "y = x.sum()\n",
    "print(x.collect())\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 4, 1]\n",
      "2.3333333333333335\n"
     ]
    }
   ],
   "source": [
    "# Mean Return the mean of the items in the RDD\n",
    "x = spark.sparkContext.parallelize([2,4,1])\n",
    "y = x.mean()\n",
    "print(x.collect())\n",
    "print(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 4, 1]\n",
      "1.247219128924647\n"
     ]
    }
   ],
   "source": [
    "# stdev Return the standard deviation of the items in the RDD\n",
    "x = spark.sparkContext.parallelize([2,4,1])\n",
    "y = x.stdev()\n",
    "print(x.collect())\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {'J': 2, 'F': 1, 'A': 1})\n"
     ]
    }
   ],
   "source": [
    "# countByKey Return a map of keys and counts of their occurrences in the RDD\n",
    "x = spark.sparkContext.parallelize([('J', 'James'), ('F','Fred'),\n",
    "('A','Anna'), ('J','John')])\n",
    "y = x.countByKey()\n",
    "print(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
