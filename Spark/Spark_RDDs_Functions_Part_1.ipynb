{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark=SparkSession.builder.appName(\"RDD Trasformation & Actions\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAP Return a new RDD by applying a function to each element of this RDD\n",
    "x = spark.sparkContext.parallelize([\"b\", \"a\", \"c\"])\n",
    "y = x.map(lambda z: (z, 1))\n",
    "print(x.collect())\n",
    "print(y.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter Return a new RDD containing only the elements that satisfy a predicate\n",
    "x = spark.sparkContext.parallelize([1,2,3])\n",
    "y = x.filter(lambda x: x%2 == 0) #keep even values\n",
    "print(x.collect())\n",
    "print(y.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FlatMap Return a new RDD by first applying a function to all elements of this RDD, and then flattening the results\n",
    "x = spark.sparkContext.parallelize([1,2,3])\n",
    "y = x.flatMap(lambda x: (x, x*100, 42))\n",
    "print(x.collect())\n",
    "print(y.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Group By Group the data in the original RDD. Create pairs where the key is the output of\n",
    "# a user function, and the value is all items for which the function yields this key.\n",
    "x = spark.sparkContext.parallelize(['John', 'Fred', 'Anna', 'James'])\n",
    "y = x.groupBy(lambda w: w[0])\n",
    "print([(k, list(v)) for (k, v) in y.collect()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group By Key  Group the values for each key in the original RDD. Create a new pair where the\n",
    "# original key corresponds to this collected group of values.\n",
    "x = spark.sparkContext.parallelize([('B',5),('B',4),('A',3),('A',2),('A',1)])\n",
    "y = x.groupByKey()\n",
    "print(x.collect())\n",
    "print(list((j[0], list(j[1])) for j in y.collect()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MapPartitions Return a new RDD by applying a function to each partition of this RDD\n",
    "x = spark.sparkContext.parallelize([1,2,3], 2)\n",
    "def f(iterator): yield sum(iterator); yield 42\n",
    "y = x.mapPartitions(f)\n",
    "# glom() flattens elements on the same partition\n",
    "print(x.glom().collect())\n",
    "print(y.glom().collect())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Return a new RDD by applying a function to each partition of this RDD,\n",
    "#while tracking the index of the original partition\n",
    "x = spark.sparkContext.parallelize([1,2,3], 2)\n",
    "def f(partitionIndex, iterator): yield (partitionIndex, sum(iterator))\n",
    "y = x.mapPartitionsWithIndex(f)\n",
    "# glom() flattens elements on the same partition\n",
    "print(x.glom().collect())\n",
    "print(y.glom().collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample Return a new RDD containing a statistical sample of the original RDD\n",
    "x = spark.sparkContext.parallelize([1, 2, 3, 4, 5])\n",
    "y = x.sample(False,.4,42)\n",
    "print(x.collect())\n",
    "print(y.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Union Return a new RDD containing all items from two original RDDs. Duplicates are not culled.\n",
    "x = spark.sparkContext.parallelize([1,2,3], 2)\n",
    "y = spark.sparkContext.parallelize([3,4], 2)\n",
    "z = x.union(y)\n",
    "print(z.glom().collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join Return a new RDD containing all pairs of elements having the same key in the original RDDs\n",
    "x = spark.sparkContext.parallelize([(\"a\", 1), (\"b\", 2)])\n",
    "y = spark.sparkContext.parallelize([(\"a\", 3), (\"a\", 4), (\"b\", 5)])\n",
    "z = x.join(y)\n",
    "print(z.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distinct Return a new RDD containing distinct items from the original RDD (omitting all duplicates)\n",
    "x = spark.sparkContext.parallelize([1,2,3,3,4])\n",
    "y = x.distinct()\n",
    "print(y.collect())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#coalesce Return a new RDD which is reduced to a smaller number of partitions\n",
    "x = spark.sparkContext.parallelize([1, 2, 3, 4, 5], 3)\n",
    "y = x.coalesce(2)\n",
    "print(x.glom().collect())\n",
    "print(y.glom().collect())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Key BY Create a Pair RDD, forming one pair for each item in the original RDD. The\n",
    "# pair’s key is calculated from the value via a user-supplied function.\n",
    "x = spark.sparkContext.parallelize(['John', 'Fred', 'Anna', 'James'])\n",
    "y = x.keyBy(lambda w: w[0])\n",
    "print(y.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Return a new RDD with the specified number of partitions, placing original\n",
    "#items into the partition returned by a user supplied function\n",
    "x = spark.sparkContext.parallelize([('J','James'),('F','Fred'),('A','Anna'),('J','John')], 3)\n",
    "y = x.partitionBy(2, lambda w: 0 if w[0] < 'H' else 1)\n",
    "print(x.glom().collect())\n",
    "print(y.glom().collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zip Return a new RDD containing pairs whose key is the item in the original RDD, and whose\n",
    "#value is that item’s corresponding element (same partition, same index) in a second RDD\n",
    "x = spark.sparkContext.parallelize([1, 2, 3])\n",
    "y = x.map(lambda n:n*n)\n",
    "z = x.zip(y)\n",
    "print(z.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getNumPartitions Return the number of partitions in RDD\n",
    "x = spark.sparkContext.parallelize([1,2,3], 2)\n",
    "y = x.getNumPartitions()\n",
    "print(x.glom().collect())\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect Return all items in the RDD to the driver in a single list\n",
    "x = spark.sparkContext.parallelize([1,2,3], 2)\n",
    "y = x.collect()\n",
    "print(x.glom().collect())\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce Aggregate all the elements of the RDD by applying a user function\n",
    "# pairwise to elements and partial results, and returns a result to the driver\n",
    "x = spark.sparkContext.parallelize([1,2,3,4])\n",
    "y = x.reduce(lambda a,b: a+b)\n",
    "print(x.collect())\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aggregate all the elements of the RDD by:\n",
    "#- applying a user function to combine elements with user-supplied objects,\n",
    "#- then combining those user-defined results via a second user function,\n",
    "#- and finally returning a result to the driver.\n",
    "\n",
    "seqOp = lambda data, item: (data[0] + [item], data[1] + item)\n",
    "combOp = lambda d1, d2: (d1[0] + d2[0], d1[1] + d2[1])\n",
    "x = spark.sparkContext.parallelize([1,2,3,4])\n",
    "y = x.aggregate(([], 0), seqOp, combOp)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Max Return the maximum item in the RDD\n",
    "x = spark.sparkContext.parallelize([2,4,1])\n",
    "y = x.max()\n",
    "print(x.collect())\n",
    "print(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Return the sum of the items in the RDD\n",
    "x = spark.sparkContext.parallelize([2,4,1])\n",
    "y = x.sum()\n",
    "print(x.collect())\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean Return the mean of the items in the RDD\n",
    "x = spark.sparkContext.parallelize([2,4,1])\n",
    "y = x.mean()\n",
    "print(x.collect())\n",
    "print(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stdev Return the standard deviation of the items in the RDD\n",
    "x = spark.sparkContext.parallelize([2,4,1])\n",
    "y = x.stdev()\n",
    "print(x.collect())\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# countByKey Return a map of keys and counts of their occurrences in the RDD\n",
    "x = spark.sparkContext.parallelize([('J', 'James'), ('F','Fred'),\n",
    "('A','Anna'), ('J','John')])\n",
    "y = x.countByKey()\n",
    "print(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interoperating Between DataFrames, Datasets, and RDDs\n",
    "spark.range(10).rdd.glom().collect()\n",
    "spark.range(10).toDF(\"id\").rdd.map(lambda row: row[0]).collect()\n",
    "spark.range(10).rdd.toDF().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Give name to RDD to monitor in Spark GUI\n",
    "myCollection = \"Spark The Definitive Guide : Big Data Processing Made Simple\"\\\n",
    ".split(\" \")\n",
    "words = spark.sparkContext.parallelize(myCollection, 2)\n",
    "words.setName(\"myWords\")\n",
    "words.name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This creates an RDD for which each record in the RDD represents a line in that text file or files.\n",
    "current_dir=os.getcwd()\n",
    "full_path=os.path.join(current_dir,'data','netflix_titles.csv')\n",
    "data=spark.sparkContext.textFile(full_path)\n",
    "data.take(2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Alternatively, you can read in data for which each text file should become a single record. \n",
    "spark.sparkContext.wholeTextFiles(full_path).take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sort\n",
    "words.sortBy(lambda word: len(word) * -1).take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reduce\n",
    "spark.sparkContext.parallelize(range(1, 5)).reduce(lambda x, y: x + y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count\n",
    "words.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#countapprox\n",
    "confidence = 0.95\n",
    "timeoutMilliseconds = 400\n",
    "words.countApprox(timeoutMilliseconds, confidence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The same principles apply for caching RDDs as for DataFrames and Datasets.\n",
    "words.cache()\n",
    "#We can specify a storage level as any of the storage levels in the singleton object\n",
    "words.getStorageLevel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checkpointing\n",
    "#is the act of saving an RDD to disk so that future references to this RDD point to those\n",
    "#intermediate partitions on disk rather than recomputing the RDD from its original source. This is\n",
    "#similar to caching except that it’s not stored in memory, only disk\n",
    "spark.sparkContext.setCheckpointDir(current_dir)\n",
    "words.checkpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mapPartitions - this means that we operate on a per-partition basis and allows us to perform an\n",
    "#operation on that entire partition\n",
    "words.glom().collect()\n",
    "words.mapPartitions(lambda part: [1]).glom().collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Other functions similar to mapPartitions include mapPartitionsWithIndex. With this you\n",
    "#specify a function that accepts an index (within the partition) and an iterator that goes through all\n",
    "#items within the partition.\n",
    "def indexedFunc(partitionIndex, withinPartIterator):\n",
    "    return [\"partition: {} => {}\".format(partitionIndex,\n",
    "x) for x in withinPartIterator]\n",
    "words.mapPartitionsWithIndex(indexedFunc).collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Although mapPartitions needs a return value to work properly, this next function does not.\n",
    "#foreachPartition simply iterates over all the partitions of the data.\n",
    "def f(iterator):\n",
    "    for x in iterator:\n",
    "         print(x)\n",
    "words.foreachPartition(f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
